{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fake News Detection - Training Notebook\n",
                "\n",
                "Run this notebook in Google Colab to train the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "!pip install pandas scikit-learn nltk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Import Libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import string\n",
                "import pickle\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import classification_report, accuracy_score\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "\n",
                "# Download NLTK data\n",
                "nltk.download('stopwords')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Load Dataset\n",
                "# Ensure you have uploaded 'Fake.csv' and 'True.csv' to the Colab files section.\n",
                "try:\n",
                "    df_fake = pd.read_csv(\"Fake.csv\")\n",
                "    df_true = pd.read_csv(\"True.csv\")\n",
                "    print(\"Datasets loaded successfully.\")\n",
                "except FileNotFoundError:\n",
                "    print(\"Error: 'Fake.csv' and 'True.csv' not found. Please upload them to Colab.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Preprocessing\n",
                "df_fake[\"class\"] = 0\n",
                "df_true[\"class\"] = 1\n",
                "\n",
                "# Remove last 10 rows for manual testing (optional)\n",
                "df_fake_manual_testing = df_fake.tail(10)\n",
                "for i in range(23480, 23470, -1):\n",
                "    df_fake.drop([i], axis=0, inplace=True)\n",
                "\n",
                "df_true_manual_testing = df_true.tail(10)\n",
                "for i in range(21416, 21406, -1):\n",
                "    df_true.drop([i], axis=0, inplace=True)\n",
                "\n",
                "# Merge\n",
                "df_marge = pd.concat([df_fake, df_true], axis=0)\n",
                "\n",
                "# Drop unused columns\n",
                "df = df_marge.drop([\"title\", \"subject\", \"date\"], axis=1)\n",
                "\n",
                "# Shuffle\n",
                "df = df.sample(frac=1)\n",
                "df.reset_index(inplace=True)\n",
                "df.drop([\"index\"], axis=1, inplace=True)\n",
                "\n",
                "def wordopt(text):\n",
                "    text = text.lower()\n",
                "    text = re.sub('\\[.*?\\]', '', text)\n",
                "    text = re.sub(\"\\\\W\", \" \", text)\n",
                "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
                "    text = re.sub('<.*?>+', '', text)\n",
                "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
                "    text = re.sub('\\n', '', text)\n",
                "    text = re.sub('\\w*\\d\\w*', '', text)\n",
                "    return text\n",
                "\n",
                "print(\"Cleaning text...\")\n",
                "df[\"text\"] = df[\"text\"].apply(wordopt)\n",
                "\n",
                "# Split\n",
                "x = df[\"text\"]\n",
                "y = df[\"class\"]\n",
                "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
                "print(\"Data preprocessed and split.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Vectorization and Training\n",
                "print(\"Vectorizing...\")\n",
                "vectorization = TfidfVectorizer()\n",
                "xv_train = vectorization.fit_transform(x_train)\n",
                "xv_test = vectorization.transform(x_test)\n",
                "\n",
                "print(\"Training Logistic Regression...\")\n",
                "LR = LogisticRegression()\n",
                "LR.fit(xv_train, y_train)\n",
                "\n",
                "print(\"Evaluating...\")\n",
                "pred_lr = LR.predict(xv_test)\n",
                "print(\"Accuracy:\", accuracy_score(y_test, pred_lr))\n",
                "print(classification_report(y_test, pred_lr))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Save Model\n",
                "with open('model.pkl', 'wb') as f:\n",
                "    pickle.dump(LR, f)\n",
                "\n",
                "with open('vectorizer.pkl', 'wb') as f:\n",
                "    pickle.dump(vectorization, f)\n",
                "\n",
                "print(\"Done! Download 'model.pkl' and 'vectorizer.pkl' from the files section.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}